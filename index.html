

<html>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-64559429-2', 'auto');
    ga('send', 'pageview');

  </script>
  <head>
    <title>
      Lei Ke - CMU
    </title>
    <link rel="icon" type="image/png" href="./images/cmu.png" />

    <!--<link href="css/bootstrap.min.css" rel="stylesheet">-->
    <style>
      body {
        background-color: #eee;
      }
      .font {font-family: "Times New Roman", Times, serif;}
      .main {
        width : 900px;
        margin : 10px auto;
        border-radius: 15px;
        background-color : #fff;
        padding : 40px;
        box-shadow: 0px 0px 10px #999;
        font-family: "Helvetica";>
      }
      .intro {
        display: block;
        line-height: 140%;
      }
      .caption {
        font-size: 25px;
        font-weight: normal;
        color: #000;
        font-family: Constantia, "Lucida Bright", "DejaVu Serif", Georgia, serif;
      }
      .caption1 {
        font-weight: normal;
        color: #000;
        font-family: Constantia, "Lucida Bright", "DejaVu Serif", Georgia, serif;
      }
      a {
        color: #4682B4;
        text-decoration: none;
      }
      a:hover {
        text-decoration : underline;
      }
      a:visited { 
        color : #4682B4;
      }
      img.me {
        margin-left: 5px;
        margin-right : 25px;
        border : 0 solid black;
        float : left;
        margin-bottom : 0;
        height: 240px;
        border-radius : 10px; 
      }
      div.projects {
        margin-top: 20px;
      }
            div.news {
                margin-top: 20px;
            }
            div.work {
                margin-top: 20px;
            }
      .project-thumb {
        width: 250px;
      }
      .projects-table {
        margin-top: 20px;
      }
      .description-column {
        padding-left: 30px;
        width: 4500px;
      }
      .project-title {
        font-size: 16px;
        font-weight:bold;
        padding-bottom: 5px;
      }
      .description {
        text-align:justify;
        text-justify:inter-word;
        padding-bottom: 5px;
        font-size: 15px;
        font-family: "Times New Roman", Times, serif;
      }
            .work-column {
                padding-top:5px;
                padding-top:5px;
            }
      .thumb-column {
        padding-top:10px;
        padding-bottom:10px;
      }
            table.pub_table tr {
                outline: thin dotted #666666;
            }

    </style>
             <style type="text/css">
p.normal {font-style:normal}
p.italic {font-style:italic}
p.oblique {font-style:oblique}
p.font {font-family: Georgia, serif;}
li.font {font-family: Georgia, serif;}
.ul_s {
  list-style-type: none;
  margin: 0;
  padding: 0;
  overflow: hidden;
  background-color: #e9e9e9;
}

.li_s {
  float: left;
}

.li_s1 {
  float: left;
}

li a1 {
  display: inline-block;
  color: black;
  text-align: center;
  padding: 8px 16px;
  text-decoration: none;
}

li a1:hover {
  background-color: grey;
}

</style>
</head>
  <body>
    <div class="main">
      <ul class="ul_s">
      <li class='li_s1'><a href="#home"><a1><font color="4682B4">Home</font></a1></a></li>
      <li class='li_s'><a href="#publications"><a1>Publications</a1></a></li>
      <li class='li_s'><a href="#misc"><a1>Misc</a1></a></li>
      </ul>
      <div class="intro">
        <a name="home"></a>
        <img class="me" src="./image/kelei_portrait_small.jpg">
                <!--<h2><span class="name">Lei Ke</span></h2>-->
                <p class="caption"><b>Lei Ke</b></p>
                <p class="caption1"><b> Senior Researcher at Tencent AI Seattle</b></p>
                <p class="font"> I am a Senior Research Scientist in <a target="_blank" href='https://www.tencent.com/en-us/index.html'>Tencent AI, Seattle Lab</a>. My primary research interest lies in building multimodal foundation systems, especially the visual understanding, reasoning, and generation. Previously, I worked as a Postdoctoral Research Associate at Computer Science of <a target="_blank" href='https://www.ml.cmu.edu/'>Carnegie Mellon University</a> and in the Computer Vision Lab of <a target="_blank" href='https://ethz.ch/en.html'>ETH Zurich</a>. I obtained my Ph.D. degree from <a target="_blank" href='https://www.cse.ust.hk/'>CSE Department</a> at <a target="_blank" href='https://www.ust.hk/'>HKUST</a> in mid 2023, supervised by <a target="_blank" href="http://home.cse.ust.hk/~cktang/bio.html">Chi-Keung Tang</a> and <a target="_blank" href="https://yuwingtai.github.io/">Yu-Wing Tai</a>. During the PhD journey, I also spent two years as a visiting scholar at ETH Zurich.I received my B.E. degree from the school of computer science at <a target="_blank" href='https://en.wikipedia.org/wiki/Wuhan_University'>Wuhan University</a>.</p>
                <p class="font">
                More info: [<a href="mailto:keleiwhu@gmail.com">Email</a>],[<a target="_blank" href="https://scholar.google.com/citations?user=WseeNrUAAAAJ&hl=en">Google Scholar</a>],[<a target="_blank" href="https://x.com/leike_lk/">X</a>],[<a target="_blank" href="https://github.com/lkeab/">GitHub</a>], where my opensource projects obtains over <font color = 'red'>10K+</font> GitHub stars.<br></p>
        
      </div>
      <hr>

            <div class="news">
              <h3 >Updates</h3>
              <!--<div style="overflow-y: scroll; height:150px; width:900px">-->
              <td style="border-style: none; border-width: medium;">
                <ul style="text-align:justify;height: 130px; overflow:auto;">
                    <li class="font"><font color = 'red'>2025.08</font>: We are organizing workshop <a target="_blank" href='https://awesomedigitaltwin.github.io/2025_ICCV.html'>Generating Digital Twins from Images and Videos</a> at ICCV 2025.</li>
                    <li class="font">2025.08: I will serve as an Area Chair at <a target="_blank" href='https://iclr.cc/Conferences/2026'>ICLR 2026</a>.</li>
                    <li class="font">2025.03: I will serve as an Area Chair at <a target="_blank" href='https://neurips.cc/Conferences/2025'>NeurIPS 2025</a>.</li>
                    <li class="font">2024.11: Invited <a target="_blank" href='https://czhang0528.github.io/teaching/csce689-fall2024'>guest lecture</a> at Texas A&M University on Vision Foundation Model.</li>
                    <li class="font"><font>2024.04</font>: Joined CMU as a postdoc to work with Prof. <a target="_blank" href='https://www.cs.cmu.edu/~katef/'>Katerina Fragkiadaki</a>, and if you're passionate about vision / robotics with extensive research experience, feel free to reach out for research collaboration!</li>
                    <li class="font">2023.11: We released the <a target="_blank" href='https://github.com/lkeab/gaussian-grouping'>Gaussian Grouping</a> project.</li>
                    <li class="font">2023.11: Talks on <a target="_blank" href='https://andrewcmu-my.sharepoint.com/:p:/g/personal/leik_andrew_cmu_edu/EdVoubQ3wtlAvzeqOyJs-_UBjdg-1wliv_5Y4JySM1AynA?e=OygszK'>Scene Understanding with Vision Foundation Models</a> at <a target="_blank" href='https://svl.stanford.edu/'>Stanford SVL</a> and <a target="_blank" href='https://marvl.stanford.edu/index.html'>MARVL</a>.</li>
                    <li class="font">2023.06: We released the <a target="_blank" href='https://github.com/SysCV/sam-hq'>HQ-SAM</a> and <a target="_blank" href='https://github.com/SysCV/sam-pt'>SAM-PT</a> projects. </li>
                    <li class="font">2023.06: Technical committee of <a target="_blank" href='https://www.votchallenge.net/vots2023/people.html'>VOTS 2023 Challenge</a>. The workshop will take place on October 3rd @ICCV2023. </li>
                    <li class="font">2023.05: Passed the <a target="_blank" href='https://calendar.hkust.edu.hk/zh-hans/events/phd-computer-science-engg-image-and-video-instance-segmentation-towards-better-quality'>PhD thesis defense</a> and become a Dr.!</li>
                    <li class="font">2023.03: <a target="_blank" href='https://arxiv.org/abs/2303.15904'>Mask-free VIS</a> is accepted in CVPR 2023.</li>
                    <li class="font">2023.01: <a target="_blank" href='https://ieeexplore.ieee.org/document/10048550'>BCNet</a> is accepted in TPAMI 2023.</li>
                    <li class="font">2022.07: <a target="_blank" href='https://arxiv.org/abs/2207.14012'>Video Mask Transfiner</a> on high-quality VIS is accepted by ECCV 2022! We released the <a target="_blank" href='https://www.vis.xyz/data/hqvis/'>HQ-YTVIS</a> dataset.</li>
                    <li class="font">2022.03: <a target="_blank" href='https://www.vis.xyz/pub/pcan/'>PCAN</a> serves as the baseline in <a target="_blank" href='https://www.bdd100k.com/challenges/cvpr2022/'>BDD100K MOTS challenge</a> at <a target="_blank" href='https://cvpr2022.wad.vision'>CVPR 2022 Workshop on Autonomous Driving</a>. </li>
                    <li class="font">2022.03: <a target="_blank" href='https://arxiv.org/abs/2111.13673'>Mask Transfiner</a> on high-quality instance segmentation is accepted by CVPR 2022! </li>
                    <li class="font">2022.03: <a target="_blank" href='https://mp.weixin.qq.com/s/KDDBDrmVJAN42SPlVxsVNw'>Invited talk on PCAN</a> at AI Time Seminar, Tsinghua Univ (Virtual).</li>
                    <li class="font">2022.02: <a target="_blank" href='https://www.techbeat.net/talk-info?id=631'>Invited talk on Mutltiple Object Tracking & Segmentation in Autonomous Driving</a> at TechBeat.</li>
                    <li class="font">2021.12: <a target="_blank" href='https://www.epfl.ch/research/domains/cis/the-conference-on-neural-information-processing-systems/'>Invited spotlight talk</a> for PCAN at <a target="_blank" href='https://www.stcc.ch'>SwissTech Convention Center, EPFL</a>.</li>
                    <li class="font">2021.10: <a target="_blank" href='https://www.vis.xyz/pub/pcan/'>PCAN</a> for Multiple Object Tracking and Segmentation is accepted by NeurIPS 2021 as spotlight.</li>
                    <li class="font">2021.07: Our paper on occlusion-aware video inpainting accepted by ICCV 2021.</li>
                    <li class="font">2021.01: I joined <a target="_blank" href='https://vision.ee.ethz.ch/'>CVL</a> at <a target="_blank" href='https://ethz.ch/en.html'>ETHz</a> as a visiting PhD student supervised by <a target="_blank" href='https://www.yf.io'>Prof.Fisher Yu</a> and <a target="_blank" href='https://martin-danelljan.github.io'>Dr.Martin Danelljan</a>.</li> 
                    <li class="font">2021.03: Our paper <a target="_blank" href='https://github.com/lkeab/BCNet'>BCNet</a> on occlusion-aware instance segmentation accepted by CVPR 2021!</font></li>
                    <li class="font">2021.10: Passed the <a target="_blank" href='./papers/pqe.pdf'>PhD Qualifying Exam</a>.</li> 
                    <li class="font">2020.07: Two papers (GSNet and CPMask) accepted by ECCV 2020.</li>
                    <li class="font">2020.02: Our paper on 3D human pose estimation has been accepted by CVPR 2020 for oral presentation.</li>
                    <li class="font">2019.07: Our paper on image captioning accepted by ICCV 2019.</li>
                    <li class="font">2019.05: I will start my Ph.D study at CSE, HKUST this autumn. </li>
                    <li class="font">2019.02: Our paper on video captioning accepted by CVPR 2019.  </li>
                </ul>  
              </td>
              <!--</div>-->
            </div>
       <hr>
      <div class="projects">
        <a name="publications"></a>
        <h3>Selected Publications</h3>
        Segmentation and Tracking:
        <div class="projects-table">
        <table> 
            <tr>
                    <td class="thumb-column">
                        <a href="#"><img class="project-thumb" src="images/hq-sam.gif" height="230" border="1"></a>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Segment Anything in High Quality</b>
                        </div>
                        <a target="_blank" href="https://arxiv.org/abs/2306.01567">NeurIPS 2023</a>
                        <div class="description">
                            <b>Lei Ke*</b>, Mingqiao Ye*, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu<br>
                            (* denotes equal contribution) <br>
                            We propose HQ-SAM to upgrade SAM for high-quality zero-shot segmentation.<br>
                            <b><font color = 'red'>HQ-SAM is available in <a target="_blank" href='https://huggingface.co/docs/transformers/main/model_doc/sam_hq'>Huggingface Transformers</a></font></b>
                           <br>
                        </div>
                        <div>
                            [<a target="_blank" href='https://arxiv.org/abs/2306.01567'>Paper</a>]
                            [<a target="_blank" href='https://github.com/SysCV/sam-hq'>Project</a>]
                            [<a target="_blank" href='./papers/Segment_Anything_in_High_Quality.pdf'>Pdf</a>]
                            <iframe src="https://ghbtns.com/github-btn.html?user=SysCV&repo=sam-hq&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                        </div>      
                    </div>
                </tr>
            <tr>
                <td class="thumb-column">
                    <a href="#"><img class="project-thumb" src="images/gaussian_grouping.gif" border="1"></a>
                </td>
                <td class="description-column">
                <div>
                    <div class="project-title">
                        <b>Gaussian Grouping: Segment and Edit Anything in 3D Scenes</b>
                    </div>
                    <a target="_blank" href="https://arxiv.org/abs/2312.00732">ECCV 2024</a>
                    <div class="description">
                        Mingqiao Ye, Martin Danelljan, Fisher Yu, <b>Lei Ke</b> <span>&#9824;</span><br>
                        (<span>&#9824;</span> denotes Project Lead) <br>
                        Gaussian Grouping for open-world 3D Anything reconstruction, segmentation and editing.<br>
                    </div>
                    <div>
                        [<a target="_blank" href='https://arxiv.org/abs/2312.00732'>Paper</a>]
                        [<a target="_blank" href='https://github.com/lkeab/gaussian-grouping'>Project</a>]
                        [<a target="_blank" href='https://ymq2017.github.io/gaussian-grouping/'>Web</a>]
                        <iframe src="https://ghbtns.com/github-btn.html?user=lkeab&repo=gaussian-grouping&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                    </div>      
                </div>
            </tr>
            <tr>
                <td class="thumb-column">
                    <a href="#"><img class="project-thumb" src="images/masa.gif" border="1"></a>
                </td>
                <td class="description-column">
                <div>
                    <div class="project-title">
                        <b>Matching Anything By Segmenting Anything</b>
                    </div>
                    <a target="_blank" href="https://arxiv.org/abs/2406.04221">CVPR 2024</a>
                    <div class="description">
                        Siyuan Li, <b>Lei Ke</b>, Martin Danelljan, Luigi Piccinelli, Mattia Segu, Luc Van Gool, Fisher Yu <br>
                        MASA provides a universal instance appearance model for matching any objects in any domain.<br>
                        <b><font color = 'red'>Highlight (~3% acceptance rate)</font></b>
                    </div>
                    <div>
                        [<a target="_blank" href='https://arxiv.org/abs/2406.04221'>Paper</a>]
                        [<a target="_blank" href='https://matchinganything.github.io/'>Project</a>]
                        <iframe src="https://ghbtns.com/github-btn.html?user=siyuanliii&repo=masa&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                    </div>      
                </div>
            </tr>
             
               <tr>
                    <td class="thumb-column">
                        <a href="#"><img class="project-thumb" src="images/sam-pt-new.gif" border="1"></a>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Segment Anything Meets Point Tracking</b>
                        </div>
                        <a target="_blank" href="https://arxiv.org/abs/2307.01197">arXiv</a>
                        <div class="description">
                            Frano Rajič, <b>Lei Ke</b>, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, Fisher Yu<br>
                            We propose SAM-PT to extend SAM to zero-shot video segmentation with point-based tracking.<br>
                            <b><font color = 'red'>SAM-PT receives 500+ Github stars in one week.</font></b>
                           <br>
                        </div>
                        <div>
                            [<a target="_blank" href='https://arxiv.org/abs/2307.01197'>Paper</a>]
                            [<a target="_blank" href='https://www.vis.xyz/pub/sam-pt/'>Project</a>]
                            <iframe src="https://ghbtns.com/github-btn.html?user=SysCV&repo=sam-pt&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                        </div>      
                    </div>
                </tr>
                                <tr>
                    <td class="thumb-column">
                        <a href="#"><img class="project-thumb" src="images/cvpr22_transfiner-min.png" border="1"></a>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Mask Transfiner for High-Quality Instance Segmentation</b>
                        </div>
                        <a target="_blank" href="http://cvpr2022.thecvf.com/">CVPR 2022</a>
                        <div class="description">
                            <b>Lei Ke</b>, Martin Danelljan, Xia Li, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu<br>
                            An efficient transformer-based method for highly accurate instance segmentation.<br>
                            <b><font color = 'red'>Transfiner receives 300+ Github stars in 3 months.</font></b>
                        </div>
                        <div>
                            [<a target="_blank" href='https://openaccess.thecvf.com/content/CVPR2022/papers/Ke_Mask_Transfiner_for_High-Quality_Instance_Segmentation_CVPR_2022_paper.pdf'>Paper</a>]
                            [<a target="_blank" href='https://arxiv.org/abs/2111.13673'>arXiv</a>]
                            [<a target="_blank" href='https://github.com/SysCV/transfiner'>Project</a>]
                            [<a target="_blank" href='https://www.vis.xyz/pub/transfiner/'>Website</a>]
                            [<a target="_blank" href='https://www.youtube.com/watch?v=YCaUclQQFoM'>Youtube</a>]
                            [<a target="_blank" href='https://exchange.scale.com/public/blogs/research-review-mask-transfiner-for-high-quality-instance-segmentation'>Blog</a>]
                            [<a target="_blank" href='https://zhuanlan.zhihu.com/p/491030678'>Zhihu</a>]
                            <iframe src="https://ghbtns.com/github-btn.html?user=SysCV&repo=transfiner&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                        </div>      
                    </div>
                </tr>

                <tr>
                    <td class="thumb-column">
                        <a href="#"><img class="project-thumb" src="images/pcan_page_demo.gif" border="1"></a>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation</b>
                        </div>
                        <a target="_blank" href="https://nips.cc/Conferences/2021">NeurIPS 2021</a>
                        <div class="description">
                            <b>Lei Ke</b>, Xia Li, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu<br>
                            Efficient cross-attention on space-time memory for video instance segmentation.<br>
                            <b><font color = 'red'>Spotlight (3% acceptance rate). PCAN receives 200+ Github stars in one month.</font></b>
                        </div>
                        <div>
                            [<a target="_blank" href='https://papers.nips.cc/paper/2021/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf'>Paper</a>]
                            [<a target="_blank" href='https://arxiv.org/abs/2106.11958'>arXiv</a>]
                            [<a target="_blank" href='https://www.vis.xyz/pub/pcan/'>Project</a>]
                            [<a target="_blank" href='https://www.youtube.com/watch?v=hhAC2H0fmP8'>Youtube</a>]
                            [<a target="_blank" href='./poster/pcan_poster.pdf'>Poster</a>]
                            [<a target="_blank" href='https://zhuanlan.zhihu.com/p/445457150'>Zhihu</a>]
                            <iframe src="https://ghbtns.com/github-btn.html?user=SysCV&repo=pcan&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                        </div>
                    </div>
                </tr>
                <tr>
                    <td class="thumb-column">
                        <a href="#"><img class="project-thumb" src="images/cvpr2021_bcn.png" border="1"></a>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers</b>
                        </div>
                        <a target="_blank" href="http://cvpr2021.thecvf.com/">CVPR 2021 & TPAMI 2023</a>
                        <div class="description">
                            <b>Lei Ke</b>, Yu-Wing Tai, Chi-Keung Tang <br>
                            Instance segmentation with bilayer decoupling structure for occluder & occludee.<br>
                            <b><font color = 'red'>BCNet receives 300+ Github stars in 6 months.</font></b>
                        </div>
                        <div>
                            [<a target="_blank" href='https://openaccess.thecvf.com/content/CVPR2021/papers/Ke_Deep_Occlusion-Aware_Instance_Segmentation_With_Overlapping_BiLayers_CVPR_2021_paper.pdf'>Paper</a>]
                            [<a target="_blank" href='https://arxiv.org/abs/2103.12340'>arXiv</a>]
                            [<a target="_blank" href='https://www.computer.org/csdl/journal/tp/5555/01/10048550/1KQ5FWfJ43K'>TPAMI Link</a>]
                            [<a target="_blank" href='https://github.com/lkeab/BCNet'>Project</a>]
                            [<a target="_blank" href='https://zhuanlan.zhihu.com/p/378269087'>Zhihu</a>]
                            [<a target="_blank" href='https://www.youtube.com/watch?v=iHlGJppJGiQ'>Video</a>]
                            [<a target="_blank" href='./poster/BCNet_CVPR21.pdf'>Poster</a>] 
                            <iframe src="https://ghbtns.com/github-btn.html?user=lkeab&repo=BCNet&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                        </div>      
                    </div>
                </tr>
                </table>
            </div>
            3D Reconstruction, Tracking and Generation:
            <div class="projects-table">
        <table> 
            <tr>
                <td class="thumb-column">
                    <a href="#"><img class="project-thumb" src="images/tapip3d_demo.gif" border="1"></a>
                </td>
                <td class="description-column">
                <div>
                    <div class="project-title">
                        <b>TAPIP3D: Tracking Any Point in Persistent 3D Geometry</b>
                    </div>
                    <a target="_blank" href="https://arxiv.org/abs/2504.14717">NeurIPS 2025</a>
                    <div class="description">
                        Bowei Zhang*, <b>Lei Ke*</b>, Adam W. Harley, Katerina Fragkiadaki <br>
                            (* denotes equal contribution) <br>
                        <b>TAPIP3D</b>: Long-term feed-forward 3D point tracking in persistent 3D point maps.<br>
                    </div>
                    <div>
                        [<a target="_blank" href='http://arxiv.org/abs/2504.14717'>Paper</a>]
                        [<a target="_blank" href='https://tapip3d.github.io/'>Project</a>]
                        <iframe src="https://ghbtns.com/github-btn.html?user=zbw001&repo=TAPIP3D&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                    </div>      
                </div>
            </tr>
            <tr>
                <td class="thumb-column">
                    <a href="#"><img class="project-thumb" src="images/rollingdepth.gif" border="1"></a>
                </td>
                <td class="description-column">
                <div>
                    <div class="project-title">
                        <b>Video Depth without Video Models</b>
                    </div>
                    <a target="_blank" href="http://arxiv.org/abs/2411.19189">CVPR 2025</a>
                    <div class="description">
                        Bingxin Ke, Dominik Narnhofer, Shengyu Huang, <b>Lei Ke</b>, Torben Peters, Katerina Fragkiadaki, Anton Obukhov, Konrad Schindler <br>
                        <b>RollingDepth</b>: A universal monocular depth estimator for arbitrarily long videos.<br>
                    </div>
                    <div>
                        [<a target="_blank" href='http://arxiv.org/abs/2411.19189'>Paper</a>]
                        [<a target="_blank" href='https://rollingdepth.github.io/'>Project</a>]
                        [<a target="_blank" href='https://huggingface.co/spaces/prs-eth/rollingdepth'>HuggingFace</a>]
                        <iframe src="https://ghbtns.com/github-btn.html?user=prs-eth&repo=rollingdepth&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                    </div>      
                </div>
            </tr>
           <tr>
                <td class="thumb-column">
                    <a href="#"><img class="project-thumb" src="images/dreamscene4d.gif" border="1"></a>
                </td>
                <td class="description-column">
                <div>
                    <div class="project-title">
                        <b>DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos</b>
                    </div>
                    <a target="_blank" href="https://arxiv.org/abs/2405.02280">NeurIPS 2024</a>
                    <div class="description">
                        Wen-Hsuan Chu*, <b>Lei Ke*</b>, Katerina Fragkiadaki <br>
                        DreamScene4D for generating 3D dynamic scenes of multiple objects from monocular videos.<br>
                    </div>
                    <div>
                        [<a target="_blank" href='https://arxiv.org/abs/2405.02280'>Paper</a>]
                        [<a target="_blank" href='https://github.com/dreamscene4d/dreamscene4d'>Project</a>]
                        [<a target="_blank" href='https://dreamscene4d.github.io/'>Web</a>]
                        <iframe src="https://ghbtns.com/github-btn.html?user=dreamscene4d&repo=dreamscene4d&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                    </div>      
                </div>
            </tr>
                            </table>
            </div>
          <hr>
      </div>
            <div class="work">
                <h3>Experiences</h3>
                <table>
                    <tr>
                        <td class="work-column">
                            <img src="images/logo-cmu.jpeg" width=40>
                        </td>
                        <td>
                            <p class="font" style="font-size: 16px">2024.04&mdash;2025.03: <a target="_blank" href='https://www.ml.cmu.edu/people/postdocs-project-scientists.html'>Postdoc</a> at MLD, CMU</p>
                        </td>
                   </tr>
                <tr>
                        <td class="work-column">
                            <img src="images/eth.png" width=40>
                        </td>
                        <td>
                            <p class="font" style="font-size: 16px">2023.07&mdash;2024.03: <a target="_blank" href='https://ee.ethz.ch/the-department/people-a-z/person-detail.MjkxNTE1.TGlzdC8zMjc5LC0xNjUwNTg5ODIw.html'>Postdoc</a> at CVL, ETHz</p>
                        </td>
                   </tr>
                   <tr>
                        <td class="work-column">
                            <img src="images/eth.png" width=40>
                        </td>
                        <td>
                            <p class="font" style="font-size: 16px">2021.01&mdash;2023.03: <a target="_blank" href='https://ee.ethz.ch/the-department/people-a-z/person-detail.MjkxNTE1.TGlzdC8zMjc5LC0xNjUwNTg5ODIw.html'>Visiting PhD student</a> at CVL, ETHz</p>
                        </td>
                   </tr>
                   <tr>
                        <td class="work-column">
                            <img src="images/ust.png" width=38>
                        </td>
                        <td>
                            <p class="font" style="font-size: 16px">2019.05&mdash;2023.05: HKUST Computer Vision Research Assistant</p>
                        </td>
                   </tr>
                   <tr>
                        <td class="work-column">
                            <img src="images/logo-youtu.png" width=35>
                        </td>
                        <td>
                            <p class="font" style="font-size: 16px">2017.11&mdash;2019.11: <a target="_blank" href="https://ai.qq.com/hr/youtu.shtml">Tencent Youtu X-lab</a> Computer Vision Research Intern, worked closely with <a target="_blank" href="https://wenjiepei.github.io/">Wenjie Pei</a>.</p>
                        </td>
                    </tr>
                    <tr>
                        <td class="work-column">
                            <img src="images/logo-alibaba.png" width=35>
                        </td>
                        <td>
                            <p class="font" style="font-size: 16px">2017.05&mdash;2017.10: <a target="_blank" href="https://www.alibabagroup.com/en/global/home">Alibaba</a> Engineering Intern</p>
                        </td>
                    </tr>
                    <tr>
                        <td class="work-column" width=50>
                            <img src="images/logo-whu.png" width=35>
                        </td>
                        <td>
                            <p class="font" style="font-size: 16px">2016.05&mdash;2017.02: Undergraduate Research Assistant at Wuhan University</p>
                        </td>
                    </tr>
                </table>
            </div>
      <div class="Miscellaneous">
      <!--  <h3>Previous Publications</h3>
        <div class="projects-table">
        <table>
                <tr>
                    <td>
                    <div>
                        <ul><li>
                        <div class="project-title">
                            <I>Monitoring Environmental Quality by Sniffing Social Media</I>
                        </div>
                        <a target="_blank" href="https://www.mdpi.com/journal/sustainability">Sustainability, 2017</a>
                        <div class="description">
                            Wang Z, <b>Lei Ke</b>, Cui X, et al. <br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://www.mdpi.com/2071-1050/9/2/85">Paper</a>]
                        </div>
                        <div>
                        </div>
                    </li></ul>
                    </td>
                </tr>
                <tr>
                    <td>
                    <div>
                        <ul><li>
                        <div class="project-title">
                            <I>A hybrid model of sentimental entity recognition on mobile social media</I>
                        </div>
                        <a target="_blank" href="https://jwcn-eurasipjournals.springeropen.com/">EURASIP Journal on Wireless Communications and Networking, 2016</a>
                        <div class="description">
                            Wang, Z., Cui, X., Gao, L., Yin, Q., <b>Lei Ke</b>, & Zhang, S.</b>
                        </div>
                        <div>
                            [<a target="_blank" href="https://jwcn-eurasipjournals.springeropen.com/articles/10.1186/s13638-016-0745-7">Paper</a>]
                        </div>
                    </div>
                    </li></ul>
                    </td>
                </tr> 
        </table>
                    </div>
        </div>-->
            <hr>
            <h3>Awards</h3>
            <div>
                2023, Received <a href=https://cvpr2023.thecvf.com/Conferences/2023/CallForDoctoralConsortium target='_blank'>CVPR Doctoral Consortium Awards(13% success rate)</a>, supervised by <a href=http://people.cs.uchicago.edu/~mmaire/ target='_blank'>Michael Maire</a>.
            </div>
            <div>
            2022, Most Popular Speakers in <a href=https://mp.weixin.qq.com/s/GfeYJ6zpGYaVHs2kKpjQcA target="_blank">TechBeat</a>
            </div>
           <div>
            2022, Research Travel Grant, ETH Zürich
            </div>
            <div>
            2019, Research Travel Grant, HKUST
            </div>
            <div>
            2019 - Present, Postgraduate Studentship, HKUST
            </div>
            <div>
            <a href=http://www.comap-math.com/mcm/2017Certs/55832.pdf target="_blank">2017, COMAP's Mathematical Contest in Modeling, Honorable prize</a>
            </div>
            <div>
                2015 - 2017, Excellent Student Scholarship, Wuhan University
            </div>
            <div>
                2016, National Software Design Competition, Second Prize
            </div>
            <div>
                2016, National Inspirational Scholarship, Wuhan University
            </div>
            <div>
                2015, National College Students' Mathematics Competition, Third Prize
            </div>
                <hr>
            <h3>Professional Activities</h3>
            <ul>
                <li>Area Chair</li>
                    NeurIPS 2025, ICLR 2026.
            </ul>
            <ul>
                <li>Conference Review</li>
                    CVPR 2020/2021/2022/2023, ICCV 2021, ECCV 2022, NeurIPS 2020/2021/2022/2023, ICML 2021/2022/2023, ICLR 2022/2023, ICRA 2022.
            </ul>
            <ul>
                <li>Journal Review</li>
                    TPAMI, IJCV, RA-L.
            </ul>
            <ul>
                <li>Teaching Assistant</li>
                    Computer Graphics (COMP4411), Spring, 2019-2020. <br>
                    Introduction to Object-oriented Programming (COMP2011), Fall, 2020-2021.
            </ul>
            <hr>
            <h3>Misc</h3>
            <a name="misc"></a>
            <ul>
            <li>Some <a href='https://500px.com/keleiwhu' target="_blank">photography</a> of natural scenery and delicate architecture during my daily life and travelling.</li>
                </ul>
            
    </div>
    
    <!--<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=YICm8uIZPVi9gwrXwFmvWE6xuRmzKIeq4BA7tA-JH-g"></script>-->

        
  </body>
</html>
